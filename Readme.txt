# DocuChat: Document-Based LLM with Reinforcement Learning

A chatbot system that learns from your documents and improves through reinforcement learning, inspired by "Reinforcement Learning for Reasoning in Small LLMs."

<p align="center">
  <pre>
  +-------------------------------------------+
  |                                           |
  |              D O C U C H A T              |
  |                                           |
  |     Document-Based LLM with RL Capabilities    |
  |                                           |
  +-------------------------------------------+
  </pre>
</p>

## ğŸŒŸ Features

- **Document Processing** - Upload PDFs, TXTs, DOCXs, and extract text efficiently
- **Semantic Search** - Use advanced embeddings to retrieve relevant information from documents
- **Small LLM Integration** - Efficient use of small language models (1.5B-7B parameters)
- **Reinforcement Learning** - Improve model performance through user feedback
- **Web Interface** - User-friendly UI for document management and chatting
- **API Endpoints** - RESTful API for integration with other applications
- **Optimized Context Window** - Smart context retrieval to maximize relevance
- **Reasoning Enhancement** - Uses GRPO algorithm to improve model reasoning

## ğŸ“‹ Requirements

- Python 3.8+
- 8GB+ RAM (for small LLMs)
- CUDA-compatible GPU (recommended but not required)

## ğŸš€ Installation

### Clone the Repository

```bash
git clone https://github.com/yourusername/docuchat.git
cd docuchat
```

### Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Install Dependencies

```bash
pip install -e .
```

### Set Up Data Directories

```bash
mkdir -p data/documents data/processed data/training
```

## ğŸ› ï¸ Usage

### Start the API Server

```bash
python scripts/run_api.py
```

The API server will start at http://localhost:8000 by default.

### Start the UI

```bash
python scripts/run_ui.py
```

The UI will be available at http://localhost:8501 by default.

### Train the Model

```bash
python scripts/train_model.py --model_name "TinyLlama/TinyLlama-1.1B-Chat-v1.0" --steps 100
```

### Evaluate the Model

```bash
python scripts/evaluate_model.py --model_name "data/trained_models/checkpoint-100" --test_set "data/training/eval_data.json"
```

## ğŸ“š How It Works

### Document Processing Flow

1. **Upload** - Documents are uploaded through the UI or API
2. **Processing** - Text is extracted, cleaned, and chunked
3. **Embedding** - Document chunks are embedded using a sentence transformer
4. **Storage** - Embeddings and metadata are stored for retrieval

### Chat Flow

1. **Query** - User sends a question
2. **Retrieval** - Relevant document chunks are retrieved
3. **Context Building** - Chunks are formatted into a context prompt
4. **Generation** - The LLM generates a response based on the context and query
5. **Feedback** - User provides feedback to improve the model

### Reinforcement Learning

The system uses Group Relative Policy Optimization (GRPO) as described in the paper "Reinforcement Learning for Reasoning in Small LLMs." This approach:

1. Generates multiple outputs for each prompt
2. Calculates rewards for each output
3. Uses relative advantages within the group to update the model policy
4. Avoids the need for a separate critic model, saving computational resources

## ğŸ”„ Reinforcement Learning Process

<p align="center">
  <pre>
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                 â”‚     â”‚                â”‚     â”‚                â”‚     â”‚                 â”‚
  â”‚ Document Upload â”œâ”€â”€â”€â”€â–ºâ”‚ User Question  â”œâ”€â”€â”€â”€â–ºâ”‚ Context        â”œâ”€â”€â”€â”€â–ºâ”‚ LLM Generation  â”‚
  â”‚                 â”‚     â”‚                â”‚     â”‚ Retrieval      â”‚     â”‚                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                 â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                 â”‚     â”‚                â”‚     â”‚                â”‚     â”‚                 â”‚
  â”‚ Model Update    â”‚â—„â”€â”€â”€â”€â”¤ GRPO Training  â”‚â—„â”€â”€â”€â”€â”¤ Reward         â”‚â—„â”€â”€â”€â”€â”¤ User Feedback   â”‚
  â”‚                 â”‚     â”‚                â”‚     â”‚ Calculation    â”‚     â”‚                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  </pre>
</p>

The RL process improves the model's reasoning capabilities by:

- **Format Learning** - Encouraging structured thinking through format rewards
- **Accuracy Improvement** - Rewarding correct answers
- **Conciseness** - Using cosine rewards to balance response length
- **User Feedback Integration** - Converting user ratings to reward signals

## ğŸ“Š Results

Our implementation achieves significant improvements with minimal resources:

- Training a 1.5B parameter model to outperform much larger models
- Requiring only ~100 training steps (compared to thousands for baseline models)
- Achieving high efficiency with only 7000 training examples
- Minimizing training cost to under $50 on consumer hardware

## ğŸ§© Project Structure

```
docuchat/
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ setup.py                # Package installation
â”œâ”€â”€ .gitignore              # Git ignore file
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml         # Configuration file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/          # Where user documents are stored
â”‚   â””â”€â”€ processed/          # Processed document embeddings
â”œâ”€â”€ docuchat/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_loader.py       # Loads LLM models
â”‚   â”‚   â”œâ”€â”€ document_processor.py # Processes documents
â”‚   â”‚   â”œâ”€â”€ embeddings.py         # Handles vector embeddings
â”‚   â”‚   â”œâ”€â”€ retriever.py          # Retrieves relevant context
â”‚   â”‚   â””â”€â”€ reward_model.py       # RL reward models
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rl_trainer.py         # RL training implementation
â”‚   â”‚   â””â”€â”€ data_handler.py       # Training data management
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py             # API endpoints
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ app.py                # Streamlit UI
â”‚       â””â”€â”€ components.py         # UI components
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_model.py            # Training script
â”‚   â””â”€â”€ evaluate_model.py         # Evaluation script
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_document_processor.py
    â”œâ”€â”€ test_retriever.py
    â””â”€â”€ test_reward_model.py
```

## ğŸ”§ Configuration

The system is configured through `config/config.yaml`. Key configurations include:

- **Model**: Base model name, embedding model, and generation parameters
- **Document Processing**: Chunk size, overlap, supported file types
- **Retrieval**: Number of chunks to retrieve, similarity threshold
- **RL Training**: Algorithm, learning rate, batch size, rewards
- **UI/API**: Host, port, theme settings

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“š References

- Quy-Anh Dang, Chris Ngo. "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't." [Preprint](https://arxiv.org/abs/2503.16219)

## ğŸ™ Acknowledgments

- Built with [HuggingFace Transformers](https://huggingface.co/docs/transformers/index)
- Uses [FAISS](https://github.com/facebookresearch/faiss) for vector similarity search
- UI powered by [Streamlit](https://streamlit.io/)
- API powered by [FastAPI](https://fastapi.tiangolo.com/)
